# -*- coding: utf-8 -*-
"""ML_Lab1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14fdVejsLOOB9IjarxeDDTQCmdykddJw7

**Analyze public sentiment on a specific niche topic (e.g., emerging technologies, niche markets, or local events) using social media data. The aim is to detect shifts in sentiment trends and uncover key drivers behind the sentiment**

•	Use a dataset related to tweets, Reddit posts, or forum discussions on a specific topic.

•	Example dataset: Sentiment140 (contains tweets with sentiment labels).

•	Alternatively, students can scrape recent social media data (if allowed).
"""

import kagglehub
import os
import pandas as pd

dataset_path = kagglehub.dataset_download("milobele/sentiment140-dataset-1600000-tweets")
train_file = os.path.join(dataset_path, 'training.1600000.processed.noemoticon.csv')

batch_size = 100000
columns = ['sentiment', 'id', 'date', 'query', 'user', 'text']
data_iterator = pd.read_csv(train_file, encoding='latin-1', names=columns, chunksize=batch_size)

"""•	Clean the text (remove hashtags, mentions, URLs, and special characters).

•	Tokenize, lemmatize, and perform sentiment analysis.
"""

import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from datetime import datetime
import re
import pandas as pd

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('vader_lexicon')

# Initializing the lemmatizer
lemmatizer = WordNetLemmatizer()

def preprocess_text(text):
    if pd.isna(text):
        return ""
    tokens = word_tokenize(text)
    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]
    return " ".join(lemmatized_words)

def preprocess_data(data):
    data = data[['sentiment', 'date', 'user', 'text']]

    # Mapping sentiment values
    data['sentiment'] = data['sentiment'].map({0: 'negative', 2: 'neutral', 4: 'positive'})

    # Cleanning the text
    data['cleaned_text'] = data['text'].apply(lambda text: re.sub(r'http\S+|@\w+|#\w+|[^A-Za-z\s]', '', str(text)).lower())

    # Processing the cleaned text
    data['processed_text'] = data['cleaned_text'].apply(preprocess_text)

    data['date'] = pd.to_datetime(data['date'], errors='coerce')

    # Handling NaN values in processed_text
    print(f"Number of NaN values in 'processed_text': {data['processed_text'].isna().sum()}")

    # Replacing NaN with an empty string
    data['processed_text'] = data['processed_text'].fillna('')
    return data

nltk.download('punkt_tab')

processed_batches = []
for chunk in data_iterator:
    processed_batches.append(preprocess_data(chunk))
train_data = pd.concat(processed_batches)

"""•	Visualize trends in sentiment over time.

•	Identify frequently mentioned topics or keywords using word clouds or topic modeling (LDA).
"""

import matplotlib.pyplot as plt

trend_data = train_data.groupby(train_data['date'].dt.date)['sentiment'].value_counts().unstack()
trend_data.plot(kind='line', figsize=(10, 6))
plt.title('Sentiment Trends Over Time')
plt.xlabel('Date')
plt.ylabel('Number of Tweets')
plt.show()

from wordcloud import WordCloud

text = " ".join(train_data['processed_text'])
wordcloud = WordCloud(stopwords='english').generate(text)
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()

"""•	Build a sentiment classifier using a machine learning algorithm like Logistic Regression, SVM, or Random Forest.

•	Evaluate the model on classification metrics (precision, recall, F1-score).

"""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X_tfidf = vectorizer.fit_transform(train_data['processed_text'])

X_train, X_test, y_train, y_test = train_test_split(
    train_data['processed_text'], train_data['sentiment'], test_size=0.2, random_state=42
)
X_train_tfidf = vectorizer.transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

classifier = LogisticRegression(max_iter=1000)
classifier.fit(X_train_tfidf, y_train)

y_pred = classifier.predict(X_test_tfidf)
print(classification_report(y_test, y_pred))

"""Saving the Model and Vectorizer"""

train_data.head()

import joblib

joblib.dump(classifier, 'logistic_regression_model.pkl')

vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
X_tfidf = vectorizer.fit_transform(train_data['processed_text'])

joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')
train_data.to_csv('processed_train_data.csv', index=False)